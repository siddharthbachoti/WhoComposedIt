{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport os\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sn\nfrom keras.models import Sequential\nfrom keras.layers import Dense, MaxPooling2D, MaxPooling1D, Conv2D, Conv1D, Flatten, Dropout, SpatialDropout1D, SimpleRNN, GRU, LSTM\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nimport random\nimport pandas as pd\nimport time\nimport pickle\nimport copy","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_time_series(data):\n    data1 = []\n    for i in data:\n        data1.append(np.transpose(i))\n    return np.array(data1)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transpose_scale(arr,num):\n    arr2 = copy.deepcopy(arr)\n    arr2 = np.roll(arr2,-num,axis = 0)\n    if int(num/abs(num))==1:\n        for i in range(num):\n            arr2[-(i+1)] = np.zeros(len(arr2[0]))\n    else:\n        for i in range(abs(num)):\n            arr2[i] = np.zeros(len(arr2[0]))\n    return arr2","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def translate_time(arr,num):\n    \n    arr2 = copy.deepcopy(arr)\n    arr2 = np.transpose(arr2)\n    arr2 = np.roll(arr2,-num,axis = 0)\n    if int(num/abs(num))==1:\n        for i in range(num):\n            arr2[-(i+1)] = np.zeros(len(arr2[0]))\n    else:\n        for i in range(abs(num)):\n            arr2[i] = np.zeros(len(arr2[0]))\n            \n    return np.transpose(arr2)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_dynamics(arr):\n    for i in np.transpose(np.nonzero(arr)):\n        arr[i[0]][i[1]] = 1\n    return arr","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# 3^{o} Classification (Bach+Mozart+Beethoven)"},{"metadata":{},"cell_type":"markdown","source":"# Reading and creating the Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading the training rolls and training labels\ntraining_rolls = []\ntraining_labels = []\nfor i in range(1202):\n    filename1 = '/kaggle/input/prollnpy3/train_rolls/train_rolls/train_roll' + str(i) + '.npy'\n    filename2 = '/kaggle/input/prollnpy3/train_labels/train_labels/train_label' + str(i) + '.npy'\n    temp1 = np.load(filename1)\n    temp2 = np.load(filename2)\n    training_rolls.append(temp1)\n    training_labels.append(temp2)\n    \nfor i in range(len(training_labels)):\n    if training_labels[i]==3:\n        training_labels[i] = 2","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading the test rolls and test labels\ntest_rolls = []\ntest_labels = []\nfor i in range(301):\n    filename1 = '/kaggle/input/prollnpy3/test_rolls/test_rolls/test_roll' + str(i) + '.npy'\n    filename2 = '/kaggle/input/prollnpy3/test_labels/test_labels/test_label' + str(i) + '.npy'\n    temp1 = np.load(filename1)\n    temp2 = np.load(filename2)\n    test_rolls.append(temp1)\n    test_labels.append(temp2)\n    \nfor i in range(len(test_labels)):\n    if test_labels[i]==3:\n        test_labels[i] = 2  ","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_rolls = np.array(training_rolls)\ntest_rolls = np.array(test_rolls)\ntraining_labels = np.array(training_labels)\ntest_labels = np.array(test_labels)","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transposition & translation"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp1 = []\ntemp2 = []\nfor i in range(len(training_rolls)):\n    temp1.append(transpose_scale(training_rolls[i],20))\n    temp1.append(transpose_scale(training_rolls[i],40))\n    temp1.append(translate_time(training_rolls[i],100))\n#     temp1.append(translate_time(training_rolls[i],-100))\n                                \n#    temp1.append(transpose_scale(training_rolls[i],-40))\n#     temp1.append(translate_time(training_rolls[i],200))\n#     temp2.append(training_labels[i])\n#     temp2.append(training_labels[i])\n#     temp2.append(training_labels[i])\n    temp2.append(training_labels[i])\n    temp2.append(training_labels[i])\n    temp2.append(training_labels[i])\n    \ntemp1 = np.array(temp1)\ntemp2 = np.array(temp2)\ntraining_rolls = np.concatenate([training_rolls,temp1])\ntraining_labels = np.concatenate([training_labels,temp2])\n\n# temp1 = []\n# temp2 = []\n# for i in range(len(test_rolls)):\n#     temp1.append(transpose_scale(test_rolls[i],4))\n#     temp1.append(transpose_scale(test_rolls[i],-4))\n#     temp1.append(translate_time(test_rolls[i],4))\n#     temp1.append(translate_time(test_rolls[i],-4))\n#     temp2.append(test_labels[i])\n#     temp2.append(test_labels[i])\n#     temp2.append(test_labels[i])\n#     temp2.append(test_labels[i])\n    \n# temp1 = np.array(temp1)\n# temp2 = np.array(temp2)\n# test_rolls = np.concatenate([test_rolls,temp1])\n# test_labels = np.concatenate([test_labels,temp2])","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Removal of dynamics"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in range(len(training_rolls)):\n#     training_rolls[i] = remove_dynamics(training_rolls[i])\n    \n# for i in range(len(test_rolls)):\n#     test_rolls[i] = remove_dynamics(test_rolls[i])","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# c = list(zip(training_rolls, training_labels))\n\n# random.shuffle(c)\n\n# training_rolls, training_labels = zip(*c)\n\n\n# d = list(zip(test_rolls, test_labels))\n\n# random.shuffle(d)\n\n# test_rolls, test_labels = zip(*d)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(training_labels)","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"3606"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Beginning the experiments"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = 3\n\npiano_roll_shape = training_rolls[0].shape\npiano_roll_size = piano_roll_shape[0]*piano_roll_shape[1]","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntraining_labels = to_categorical(training_labels, num_classes)\ntest_labels = to_categorical(test_labels, num_classes)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n(x_train,x_test) = (make_time_series(training_rolls),make_time_series(test_rolls))","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_rolls = None\ntest_rolls = None","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conv1D + GRU"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n#model.add(Embedding(2**piano_roll_shape[0], piano_roll_shape[0], input_length=piano_roll_shape[1])) \n#model.add(SpatialDropout1D(drop_embed))\nmodel.add(Conv1D(128, 4, activation='selu', input_shape=(piano_roll_shape[1],piano_roll_shape[0])))\nmodel.add(Conv1D(128, 4, activation='selu'))\nmodel.add(MaxPooling1D(4))\nmodel.add(GRU(512, activation = 'selu',dropout=0.1))\n#model.add(Conv1D(64, 4, activation='selu')\n#model.add(SimpleRNN(128, activation = 'relu', return_sequences=True, dropout=drop_rnn))\n#model.add(SimpleRNN(64, activation = 'relu', return_sequences=True, dropout=drop_rnn))\n#model.add(SimpleRNN(64, activation = 'relu', dropout=drop_rnn))\nmodel.add(Dense(64, activation='selu'))\nmodel.add(Dropout(0.1))\n# model.add(Dense(1, activation='sigmoid'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nearly_stopper = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)","execution_count":17,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv1d_1 (Conv1D)            (None, 697, 128)          36480     \n_________________________________________________________________\nconv1d_2 (Conv1D)            (None, 694, 128)          65664     \n_________________________________________________________________\nmax_pooling1d_1 (MaxPooling1 (None, 173, 128)          0         \n_________________________________________________________________\ngru_1 (GRU)                  (None, 512)               984576    \n_________________________________________________________________\ndense_1 (Dense)              (None, 64)                32832     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 3)                 195       \n=================================================================\nTotal params: 1,119,747\nTrainable params: 1,119,747\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(x_train, training_labels, batch_size=64, epochs=25, verbose=1, validation_split=.15,callbacks=[early_stopper])","execution_count":18,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Input arrays should have the same number of samples as target arrays. Found 6010 input samples and 3606 target samples.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-db0ad5d6dde3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopper\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck_array_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m                 \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_array_length_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m                 \u001b[0;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    242\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         raise ValueError('All sample_weight arrays should have '\n","\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 6010 input samples and 3606 target samples."]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accuracy  = model.evaluate(x_test, test_labels, verbose=True)\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\n\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\n\nplt.show()\n\nprint(f'Test loss: {loss:.3}')\nprint(f'Test accuracy: {accuracy:.3}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# When did our evaluator do poorly?\npredictions = model.predict(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(np.argmax(predictions, axis=1), np.argmax(test_labels, axis=1))\nplt.figure(figsize = (4, 4))\n\nname_labels = ['Bach', 'Mozart', 'Beethoven']\n\nsn.heatmap(cm, annot=True, xticklabels=name_labels, yticklabels=name_labels)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}
